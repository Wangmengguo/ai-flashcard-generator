#!/usr/bin/env python3
"""
AI Flashcard Generator åŸºå‡†æµ‹è¯•å·¥å…·
æ¯”è¾ƒä¼˜åŒ–å‰åçš„æ€§èƒ½å·®å¼‚
"""

import asyncio
import time
import json
import statistics
from typing import List, Dict, Any, Tuple
from main import (
    parse_llm_output, 
    parse_llm_output_optimized,
    FlashcardPair
)
import re

class BenchmarkTester:
    def __init__(self):
        self.test_data = []
        self.load_test_data()
    
    def load_test_data(self):
        """åŠ è½½æµ‹è¯•æ•°æ®"""
        # æ¨¡æ‹Ÿå„ç§LLMè¾“å‡ºæ ¼å¼
        self.test_data = [
            # æ ‡å‡†æ ¼å¼
            """Q: ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ
A: æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè®©è®¡ç®—æœºèƒ½ä»æ•°æ®ä¸­å­¦ä¹ ã€‚

---

Q: ç›‘ç£å­¦ä¹ çš„ç‰¹ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ
A: ç›‘ç£å­¦ä¹ ä½¿ç”¨æ ‡è®°çš„è®­ç»ƒæ•°æ®è¿›è¡Œå­¦ä¹ ã€‚""",
            
            # ä¸­æ–‡å†’å·æ ¼å¼
            """Qï¼šæ·±åº¦å­¦ä¹ çš„æ ¸å¿ƒæ˜¯ä»€ä¹ˆï¼Ÿ
Aï¼šæ·±åº¦å­¦ä¹ çš„æ ¸å¿ƒæ˜¯ç¥ç»ç½‘ç»œã€‚

---

Qï¼šCNNçš„ä¸»è¦ç”¨é€”æ˜¯ä»€ä¹ˆï¼Ÿ
Aï¼šCNNä¸»è¦ç”¨äºå›¾åƒå¤„ç†å’Œè®¡ç®—æœºè§†è§‰ä»»åŠ¡ã€‚""",
            
            # å¤æ‚æ ¼å¼
            """Q: Pythonæœ‰å“ªäº›ç‰¹ç‚¹ï¼Ÿ
A: Pythonå…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š
1. è¯­æ³•ç®€æ´æ˜“æ‡‚
2. è·¨å¹³å°æ”¯æŒ
3. ä¸°å¯Œçš„åº“ç”Ÿæ€

---

Q: ä»€ä¹ˆæ˜¯åˆ—è¡¨æ¨å¯¼å¼ï¼Ÿ
A: åˆ—è¡¨æ¨å¯¼å¼æ˜¯Pythonä¸­åˆ›å»ºåˆ—è¡¨çš„ç®€æ´æ–¹å¼ï¼Œ
è¯­æ³•ä¸ºï¼š[expression for item in iterable if condition]

---

Q: å¦‚ä½•å¤„ç†å¼‚å¸¸ï¼Ÿ
A: ä½¿ç”¨try-exceptè¯­å¥ï¼š
try:
    # å¯èƒ½å‡ºé”™çš„ä»£ç 
except Exception as e:
    # é”™è¯¯å¤„ç†""",
            
            # è¾¹ç•Œæƒ…å†µ
            """Q: ç®€å•é—®é¢˜ï¼Ÿ
A: ç®€å•ç­”æ¡ˆã€‚

---

Q: å¤šè¡Œé—®é¢˜
è¿™æ˜¯é—®é¢˜çš„ç¬¬äºŒè¡Œï¼Ÿ
A: å¤šè¡Œç­”æ¡ˆ
è¿™æ˜¯ç­”æ¡ˆçš„ç¬¬äºŒè¡Œ
è¿™æ˜¯ç­”æ¡ˆçš„ç¬¬ä¸‰è¡Œã€‚""",
            
            # å¤§é‡å¡ç‰‡
            "\n---\n".join([
                f"""Q: é—®é¢˜{i}ï¼Ÿ
A: ç­”æ¡ˆ{i}ã€‚""" for i in range(1, 21)
            ])
        ]
    
    def benchmark_parsing(self, num_iterations: int = 1000) -> Dict[str, Any]:
        """åŸºå‡†æµ‹è¯•è§£æå‡½æ•°æ€§èƒ½"""
        print(f"å¼€å§‹è§£ææ€§èƒ½åŸºå‡†æµ‹è¯• ({num_iterations} æ¬¡è¿­ä»£)...")
        
        original_times = []
        optimized_times = []
        
        original_results = []
        optimized_results = []
        
        for test_input in self.test_data:
            # æµ‹è¯•åŸå§‹å‡½æ•°
            for _ in range(num_iterations):
                start_time = time.perf_counter()
                result = parse_llm_output(test_input)
                end_time = time.perf_counter()
                original_times.append((end_time - start_time) * 1000)  # è½¬æ¢ä¸ºæ¯«ç§’
                if _ == 0:  # åªä¿å­˜ç¬¬ä¸€æ¬¡çš„ç»“æœç”¨äºéªŒè¯
                    original_results.append(result)
            
            # æµ‹è¯•ä¼˜åŒ–å‡½æ•°
            for _ in range(num_iterations):
                start_time = time.perf_counter()
                result = parse_llm_output_optimized(test_input)
                end_time = time.perf_counter()
                optimized_times.append((end_time - start_time) * 1000)  # è½¬æ¢ä¸ºæ¯«ç§’
                if _ == 0:  # åªä¿å­˜ç¬¬ä¸€æ¬¡çš„ç»“æœç”¨äºéªŒè¯
                    optimized_results.append(result)
        
        # éªŒè¯ç»“æœä¸€è‡´æ€§
        results_match = self._compare_results(original_results, optimized_results)
        
        return {
            "iterations": num_iterations,
            "test_cases": len(self.test_data),
            "results_match": results_match,
            "original_function": {
                "avg_time_ms": statistics.mean(original_times),
                "min_time_ms": min(original_times),
                "max_time_ms": max(original_times),
                "median_time_ms": statistics.median(original_times),
                "p95_time_ms": self._percentile(original_times, 0.95),
                "total_time_ms": sum(original_times)
            },
            "optimized_function": {
                "avg_time_ms": statistics.mean(optimized_times),
                "min_time_ms": min(optimized_times),
                "max_time_ms": max(optimized_times),
                "median_time_ms": statistics.median(optimized_times),
                "p95_time_ms": self._percentile(optimized_times, 0.95),
                "total_time_ms": sum(optimized_times)
            },
            "improvement": {
                "avg_speedup": statistics.mean(original_times) / statistics.mean(optimized_times),
                "total_speedup": sum(original_times) / sum(optimized_times),
                "time_saved_ms": sum(original_times) - sum(optimized_times),
                "improvement_percent": (
                    (statistics.mean(original_times) - statistics.mean(optimized_times)) / 
                    statistics.mean(original_times) * 100
                )
            }
        }
    
    def _compare_results(self, original: List[List[FlashcardPair]], 
                        optimized: List[List[FlashcardPair]]) -> bool:
        """æ¯”è¾ƒä¸¤ä¸ªç»“æœæ˜¯å¦ä¸€è‡´"""
        if len(original) != len(optimized):
            return False
        
        for orig_list, opt_list in zip(original, optimized):
            if len(orig_list) != len(opt_list):
                return False
            
            for orig_card, opt_card in zip(orig_list, opt_list):
                if orig_card.q.strip() != opt_card.q.strip() or orig_card.a.strip() != opt_card.a.strip():
                    return False
        
        return True
    
    def _percentile(self, data: List[float], percentile: float) -> float:
        """è®¡ç®—ç™¾åˆ†ä½æ•°"""
        sorted_data = sorted(data)
        index = int(len(sorted_data) * percentile)
        return sorted_data[min(index, len(sorted_data) - 1)]
    
    def benchmark_memory_usage(self) -> Dict[str, Any]:
        """åŸºå‡†æµ‹è¯•å†…å­˜ä½¿ç”¨"""
        import tracemalloc
        
        print("å¼€å§‹å†…å­˜ä½¿ç”¨åŸºå‡†æµ‹è¯•...")
        
        # æµ‹è¯•åŸå§‹å‡½æ•°å†…å­˜ä½¿ç”¨
        tracemalloc.start()
        for test_input in self.test_data * 100:  # é‡å¤100æ¬¡
            parse_llm_output(test_input)
        current, peak_original = tracemalloc.get_traced_memory()
        tracemalloc.stop()
        
        # æµ‹è¯•ä¼˜åŒ–å‡½æ•°å†…å­˜ä½¿ç”¨
        tracemalloc.start()
        for test_input in self.test_data * 100:  # é‡å¤100æ¬¡
            parse_llm_output_optimized(test_input)
        current, peak_optimized = tracemalloc.get_traced_memory()
        tracemalloc.stop()
        
        return {
            "original_peak_mb": peak_original / 1024 / 1024,
            "optimized_peak_mb": peak_optimized / 1024 / 1024,
            "memory_saved_mb": (peak_original - peak_optimized) / 1024 / 1024,
            "memory_improvement_percent": (
                (peak_original - peak_optimized) / peak_original * 100
                if peak_original > 0 else 0
            )
        }
    
    def regex_performance_test(self) -> Dict[str, Any]:
        """æµ‹è¯•æ­£åˆ™è¡¨è¾¾å¼æ€§èƒ½"""
        print("å¼€å§‹æ­£åˆ™è¡¨è¾¾å¼æ€§èƒ½æµ‹è¯•...")
        
        # æµ‹è¯•ä¸åŒçš„æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼
        patterns = {
            "simple": re.compile(r'^Q:\s*'),
            "complex": re.compile(r'^[\s\-]*[Qq][ï¼š:]?\s*', re.MULTILINE),
            "optimized": re.compile(r'^[\s\-]*[Qq][\uff1a:]?\s*', re.MULTILINE)
        }
        
        test_text = "\n".join(self.test_data) * 10  # æ‰©å¤§æµ‹è¯•æ–‡æœ¬
        
        results = {}
        iterations = 1000
        
        for pattern_name, pattern in patterns.items():
            times = []
            for _ in range(iterations):
                start_time = time.perf_counter()
                matches = pattern.findall(test_text)
                end_time = time.perf_counter()
                times.append((end_time - start_time) * 1000)
            
            results[pattern_name] = {
                "avg_time_ms": statistics.mean(times),
                "min_time_ms": min(times),
                "max_time_ms": max(times),
                "matches_found": len(matches) if matches else 0
            }
        
        return results
    
    def comprehensive_benchmark(self) -> Dict[str, Any]:
        """ç»¼åˆåŸºå‡†æµ‹è¯•"""
        print("="*60)
        print("AI Flashcard Generator æ€§èƒ½åŸºå‡†æµ‹è¯•")
        print("="*60)
        
        # è§£ææ€§èƒ½æµ‹è¯•
        parsing_results = self.benchmark_parsing()
        
        # å†…å­˜ä½¿ç”¨æµ‹è¯•
        memory_results = self.benchmark_memory_usage()
        
        # æ­£åˆ™è¡¨è¾¾å¼æ€§èƒ½æµ‹è¯•
        regex_results = self.regex_performance_test()
        
        # ç»¼åˆç»“æœ
        comprehensive_results = {
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "parsing_performance": parsing_results,
            "memory_usage": memory_results,
            "regex_performance": regex_results,
            "test_environment": {
                "test_cases": len(self.test_data),
                "total_test_text_length": sum(len(text) for text in self.test_data)
            }
        }
        
        return comprehensive_results
    
    def print_results(self, results: Dict[str, Any]):
        """æ‰“å°æµ‹è¯•ç»“æœ"""
        print("\nè§£ææ€§èƒ½æµ‹è¯•ç»“æœ:")
        print("-" * 40)
        parsing = results["parsing_performance"]
        
        print(f"æµ‹è¯•ç”¨ä¾‹æ•°: {parsing['test_cases']}")
        print(f"è¿­ä»£æ¬¡æ•°: {parsing['iterations']}")
        print(f"ç»“æœä¸€è‡´æ€§: {'âœ“' if parsing['results_match'] else 'âœ—'}")
        
        print(f"\nåŸå§‹å‡½æ•°æ€§èƒ½:")
        orig = parsing["original_function"]
        print(f"  å¹³å‡æ—¶é—´: {orig['avg_time_ms']:.4f} ms")
        print(f"  ä¸­ä½æ•°: {orig['median_time_ms']:.4f} ms")
        print(f"  95ç™¾åˆ†ä½: {orig['p95_time_ms']:.4f} ms")
        
        print(f"\nä¼˜åŒ–å‡½æ•°æ€§èƒ½:")
        opt = parsing["optimized_function"]
        print(f"  å¹³å‡æ—¶é—´: {opt['avg_time_ms']:.4f} ms")
        print(f"  ä¸­ä½æ•°: {opt['median_time_ms']:.4f} ms")
        print(f"  95ç™¾åˆ†ä½: {opt['p95_time_ms']:.4f} ms")
        
        print(f"\næ€§èƒ½æå‡:")
        imp = parsing["improvement"]
        print(f"  å¹³å‡åŠ é€Ÿ: {imp['avg_speedup']:.2f}x")
        print(f"  æ€»ä½“åŠ é€Ÿ: {imp['total_speedup']:.2f}x")
        print(f"  æ”¹è¿›ç™¾åˆ†æ¯”: {imp['improvement_percent']:.2f}%")
        print(f"  èŠ‚çœæ—¶é—´: {imp['time_saved_ms']:.2f} ms")
        
        print(f"\nå†…å­˜ä½¿ç”¨æµ‹è¯•ç»“æœ:")
        print("-" * 40)
        memory = results["memory_usage"]
        print(f"åŸå§‹å‡½æ•°å³°å€¼å†…å­˜: {memory['original_peak_mb']:.2f} MB")
        print(f"ä¼˜åŒ–å‡½æ•°å³°å€¼å†…å­˜: {memory['optimized_peak_mb']:.2f} MB")
        print(f"å†…å­˜èŠ‚çœ: {memory['memory_saved_mb']:.2f} MB")
        print(f"å†…å­˜æ”¹è¿›: {memory['memory_improvement_percent']:.2f}%")
        
        print(f"\næ­£åˆ™è¡¨è¾¾å¼æ€§èƒ½æµ‹è¯•:")
        print("-" * 40)
        regex = results["regex_performance"]
        for pattern_name, stats in regex.items():
            print(f"{pattern_name.capitalize()}æ¨¡å¼:")
            print(f"  å¹³å‡æ—¶é—´: {stats['avg_time_ms']:.4f} ms")
            print(f"  åŒ¹é…æ•°é‡: {stats['matches_found']}")

def main():
    """ä¸»å‡½æ•°"""
    tester = BenchmarkTester()
    results = tester.comprehensive_benchmark()
    
    # æ‰“å°ç»“æœ
    tester.print_results(results)
    
    # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
    with open("benchmark_results.json", "w", encoding="utf-8") as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print(f"\nè¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: benchmark_results.json")
    
    # ç”Ÿæˆç®€åŒ–æŠ¥å‘Š
    parsing = results["parsing_performance"]
    if parsing["results_match"]:
        improvement = parsing["improvement"]["improvement_percent"]
        speedup = parsing["improvement"]["avg_speedup"]
        
        print(f"\nğŸ‰ æ€§èƒ½ä¼˜åŒ–æˆåŠŸ!")
        print(f"ğŸ“ˆ å¹³å‡æ€§èƒ½æå‡: {improvement:.1f}%")
        print(f"âš¡ å¹³å‡åŠ é€Ÿæ¯”: {speedup:.1f}x")
        
        if improvement > 20:
            print("ğŸ† ä¼˜åŒ–æ•ˆæœæ˜¾è‘—!")
        elif improvement > 10:
            print("ğŸ‘ ä¼˜åŒ–æ•ˆæœè‰¯å¥½!")
        else:
            print("ğŸ“Š ä¼˜åŒ–æ•ˆæœä¸€èˆ¬")
    else:
        print("âŒ è­¦å‘Š: ä¼˜åŒ–åç»“æœä¸ä¸€è‡´ï¼Œè¯·æ£€æŸ¥ä»£ç !")

if __name__ == "__main__":
    main()